---
# yamllint disable rule:line-length
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

vars:
  destinationTemplate: "{{.ROOT_DIR}}/.taskfiles/VolSync/ReplicationDestination.tmpl.yaml"
  wipeJobTemplate: "{{.ROOT_DIR}}/.taskfiles/VolSync/WipeJob.tmpl.yaml"
  waitForJobScript: "{{.ROOT_DIR}}/.taskfiles/VolSync/wait-for-job.sh"
  listJobTemplate: "{{.ROOT_DIR}}/.taskfiles/VolSync/ListJob.tmpl.yaml"
  unlockJobTemplate: "{{.ROOT_DIR}}/.taskfiles/VolSync/UnlockJob.tmpl.yaml"
  ts: '{{now | date "150405"}}'

tasks:

  list:
    desc: List all snapshots taken by restic for a given ReplicationSource (ex. task volsync:list rsrc=vault claim=data-vault-0 [namespace=default])
    silent: true
    cmds:
      - envsubst < <(cat {{.listJobTemplate}}) | kubectl apply -f -
      - bash {{.waitForJobScript}} list-{{.rsrc}}-{{.claim}}-{{.ts}} {{.namespace}}
      - kubectl -n {{.namespace}} wait job/list-{{.rsrc}}-{{.claim}}-{{.ts}} --for condition=complete --timeout=1m
      - kubectl -n {{.namespace}} logs job/list-{{.rsrc}}-{{.claim}}-{{.ts}} --container list
      - kubectl -n {{.namespace}} delete job list-{{.rsrc}}-{{.claim}}-{{.ts}}
    vars:
      claim: '{{ or .claim (fail "ReplicationSource `claim` is required") }}'
      rsrc: '{{ or .rsrc (fail "ReplicationSource `rsrc` is required") }}'
      namespace: '{{.namespace | default "default"}}'
    env:
      claim: '{{.claim}}'
      namespace: '{{.namespace}}'
      rsrc: '{{.rsrc}}'
      ts: '{{.ts}}'
    preconditions:
      - sh: test -f {{.waitForJobScript}}
      - sh: test -f {{.listJobTemplate}}

  unlock:
    desc: Unlocks restic repository for a given ReplicationSource (ex. task volsync:unlock rsrc=vault claim=data-vault-0 [namespace=default])
    silent: true
    cmds:
      - envsubst < <(cat {{.unlockJobTemplate}}) | kubectl apply -f -
      - bash {{.waitForJobScript}} unlock-{{.rsrc}}-{{.claim}}-{{.ts}} {{.namespace}}
      - kubectl -n {{.namespace}} wait job/unlock-{{.rsrc}}-{{.claim}}-{{.ts}} --for condition=complete --timeout=1m
      - kubectl -n {{.namespace}} logs job/unlock-{{.rsrc}}-{{.claim}}-{{.ts}} --container unlock
      - kubectl -n {{.namespace}} delete job unlock-{{.rsrc}}-{{.claim}}-{{.ts}}
    vars:
      claim: '{{ or .claim (fail "ReplicationSource `claim` is required") }}'
      rsrc: '{{ or .rsrc (fail "ReplicationSource `rsrc` is required") }}'
      namespace: '{{.namespace | default "default"}}'
    env:
      claim: '{{.claim}}'
      namespace: '{{.namespace}}'
      rsrc: '{{.rsrc}}'
      ts: '{{.ts}}'
    preconditions:
      - sh: test -f {{.waitForJobScript}}
      - sh: test -f {{.unlockJobTemplate}}

  # To run backup jobs in parallel for all replicationsources:
  #  - kubectl get replicationsources --all-namespaces --no-headers | awk '{print $2, $1}' | \
  #  xargs --max-procs=4 -l bash -c 'task volsync:snapshot rsrc=$0 namespace=$1'
  #
  snapshot:
    desc: Trigger a Restic ReplicationSource snapshot (ex. task volsync:snapshot rsrc=vault claim=data-vault-0 [namespace=default])
    cmds:
      - kubectl -n {{.namespace}} patch replicationsources backup-{{.rsrc}}-{{.claim}} --type merge -p '{"spec":{"trigger":{"manual":"{{.ts}}"}}}'
      - bash {{.waitForJobScript}} volsync-src-backup-{{.rsrc}}-{{.claim}} {{.namespace}}
      - kubectl -n {{.namespace}} wait job/volsync-src-backup-{{.rsrc}}-{{.claim}} --for condition=complete --timeout=120m
      # TODO: Find a way to output logs
      # Error from server (NotFound): jobs.batch "volsync-src-zzztest" not found
      # - kubectl -n {{.namespace}} logs job/volsync-src-{{.rsrc}}
    vars:
      claim: '{{ or .claim (fail "ReplicationSource `claim` is required") }}'
      rsrc: '{{ or .rsrc (fail "ReplicationSource `rsrc` is required") }}'
      namespace: '{{.namespace | default "default"}}'
    preconditions:
      - sh: test -f {{.waitForJobScript}}
      - sh: kubectl -n {{.namespace}} get replicationsources backup-{{.rsrc}}-{{.claim}}
        msg: "ReplicationSource '{{.rsrc}}' not found in namespace '{{.namespace}}'"

  # To run restore jobs in parallel for all replicationdestinations:
  #   - kubectl get replicationsources --all-namespaces --no-headers | awk '{print $2, $1}' | \
  #   xargs --max-procs=2 -l bash -c 'task volsync:restore rsrc=$0 namespace=$1'
  #
  restore:
    desc: Trigger a Restic ReplicationSource restore (ex. task volsync:restore rsrc=vault claim=data-vault-0 [namespace=default] [previous=0])
    cmds:
      - task: restore-scale-down-app
        vars:
          controller: '{{.controller}}'
          namespace: '{{.namespace}}'
          rsrc: '{{.rsrc}}'
      - task: restore-wipe-job
        vars:
          claim: '{{.claim}}'
          namespace: '{{.namespace}}'
          rsrc: '{{.rsrc}}'
          ts: '{{.ts}}'
      - task: restore-volsync-job
        vars:
          claim: '{{.claim}}'
          fsGroup: '{{.fsGroup}}'
          namespace: '{{.namespace}}'
          previous: '{{.previous}}'
          rsrc: '{{.rsrc}}'
          runAsGroup: '{{.runAsGroup}}'
          runAsUser: '{{.runAsUser}}'
          ts: '{{.ts}}'
      - task: restore-scale-up-app-info
        vars:
          controller: '{{.controller}}'
          namespace: '{{.namespace}}'
    vars:
      claim: '{{ or .claim (fail "ReplicationSource `claim` is required") }}'
      rsrc: '{{ or .rsrc (fail "Variable `rsrc` is required") }}'
      namespace: '{{.namespace | default "default"}}'
      # 1) Query to find the controller associated with the PersistentVolumeClaim (claim)
      controller:
        sh: |
          app=$(kubectl -n {{.namespace}} get persistentvolumeclaim {{.claim}} -o jsonpath="{.metadata.labels.app\.kubernetes\.io/name}")
          if kubectl -n {{ .namespace }} get deployment.apps/$app >/dev/null 2>&1 ; then
            echo "deployment.apps/$app"
          else
            echo "statefulset.apps/$app"
          fi
      # 2) Queries to find moverSecurityContext to use when restoring
      fsGroup:
        sh: |
          kubectl -n {{.namespace}} get replicationsource {{.rsrc}}-{{.claim}} -o jsonpath="{.spec.restic.moverSecurityContext.fsGroup}"
      runAsUser:
        sh: |
          kubectl -n {{.namespace}} get replicationsource {{.rsrc}}-{{.claim}} -o jsonpath="{.spec.restic.moverSecurityContext.runAsUser}"
      runAsGroup:
        sh: |
          kubectl -n {{.namespace}} get replicationsource {{.rsrc}}-{{.claim}} -o jsonpath="{.spec.restic.moverSecurityContext.runAsGroup}"
      previous: "{{.previous | default 0}}"
    preconditions:
      - sh: test -f {{.wipeJobTemplate}}
      - sh: test -f {{.destinationTemplate}}
      - sh: test -f {{.waitForJobScript}}

  # Scale down app
  restore-scale-down-app:
    internal: true
    cmds:
      - kubectl -n {{.namespace}} scale {{.controller}} --replicas 0
      - kubectl -n {{.namespace}} wait pod --for delete --selector="app.kubernetes.io/name={{.rsrc}}" --timeout=2m

  # Wipe the PVC of all data
  restore-wipe-job:
    internal: true
    cmds:
      - envsubst < <(cat {{.wipeJobTemplate}}) | kubectl apply -f -
      - bash {{.waitForJobScript}} wipe-{{.rsrc}}-{{.claim}}-{{.ts}} {{.namespace}}
      - kubectl -n {{.namespace}} wait job/wipe-{{.rsrc}}-{{.claim}}-{{.ts}} --for condition=complete --timeout=120m
      - kubectl -n {{.namespace}} logs job/wipe-{{.rsrc}}-{{.claim}}-{{.ts}} --container wipe
      - kubectl -n {{.namespace}} delete job wipe-{{.rsrc}}-{{.claim}}-{{.ts}}
    env:
      claim: '{{.claim}}'
      namespace: '{{.namespace}}'
      rsrc: '{{.rsrc}}'
      ts: '{{.ts}}'

  # Create VolSync replicationdestination CR to restore data
  restore-volsync-job:
    internal: true
    cmds:
      - envsubst < <(cat {{.destinationTemplate}}) | kubectl apply -f -
      - bash {{.waitForJobScript}} volsync-dst-{{.rsrc}}-{{.claim}}-{{.ts}} {{.namespace}}
      - kubectl -n {{.namespace}} wait job/volsync-dst-{{.rsrc}}-{{.claim}}-{{.ts}} --for condition=complete --timeout=120m
      - kubectl -n {{.namespace}} delete replicationdestination {{.rsrc}}-{{.claim}}-{{.ts}}
    env:
      claim: '{{.claim}}'
      fsGroup: '{{.fsGroup}}'
      namespace: '{{.namespace}}'
      previous: '{{.previous}}'
      rsrc: '{{.rsrc}}'
      runAsGroup: '{{.runAsGroup}}'
      runAsUser: '{{.runAsUser}}'
      ts: '{{.ts}}'

  restore-scale-up-app-info:
    internal: true
    cmds:
      - echo -e '\n\nrestore any extra claims, and then scale the app up with:\nkubectl -n {{.namespace}} scale {{.controller}} --replicas <replicas>'

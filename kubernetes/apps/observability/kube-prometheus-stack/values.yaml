---
scrapeTargets:
  nodeExporter: []
  smartctlExporter: []
  snmpExporter: []
  zigbeeExporter: []

kube-prometheus-stack:
  crds:
    enabled: true
  cleanPrometheusOperatorObjectNames: true
  alertmanager:
    ingress:
      servicePort: 8081  # oauth-proxy
    route:
      main:
        enabled: true
        parentRefs:
          - name: internal
            namespace: kube-system
            sectionName: https
        filters:
          - type: ResponseHeaderModifier
            responseHeaderModifier:
              add:
                - name: Content-Security-Policy
                  value: >-
                    default-src 'self' 'unsafe-inline' data:; object-src 'none';
    service:
      additionalPorts:
        - name: oauth2-proxy
          port: 8081
          targetPort: 8081
          protocol: TCP
    alertmanagerSpec:
      alertmanagerConfiguration:
        name: alertmanager
        global:
          resolveTimeout: 5m
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: ceph-block
            resources:
              requests:
                storage: 64Mi
      podMetadata:
        labels:
          egress.home.arpa/authelia: allow
          egress.home.arpa/smtp-relay: allow
      additionalConfig:
        dnsConfig:
          # ensure FQDNs are not expanded with search domains, so cilium can apply policies correctly
          options:
            - name: ndots
              value: "1"

      containers:
        - name: oauth-proxy
          image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0@sha256:fe8f76728c36536d498b2a5c11172cb18dc30bbb555df2a6d14c036b29cb9f10
          envFrom:
            - secretRef:
                name: alertmanager-oidc-secret

          args:
            - --upstream=http://127.0.0.1:9093
            - --http-address=0.0.0.0:8081
            - --metrics-address=0.0.0.0:8082
          ports:
            - containerPort: 8081
              name: oauth-proxy
              protocol: TCP
            - containerPort: 8082
              name: oauth-metrics
              protocol: TCP

  kubeApiServer:
    enabled: true
  kubeControllerManager:
    enabled: true
    service:
      selector:
        component: kube-controller-manager
  kubeEtcd:
    enabled: true
    service:
      selector:
        component: kube-apiserver
  kubeProxy:  # cilium does kubeproxy
    enabled: false
  kubeScheduler:
    enabled: true
    service:
      selector:
        component: kube-scheduler
  coreDns:
    enabled: true
  kubelet:
    enabled: true

  grafana:
    enabled: false
    forceDeployDashboards: true
  kube-state-metrics:
    fullnameOverride: kube-state-metrics
    metricLabelsAllowlist:
      - pods=[*]
      - deployments=[*]
      - persistentvolumeclaims=[*]
    containerSecurityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      runAsNonRoot: true
      capabilities:
        drop:
          - ALL
    resources:
      limits:
        memory: 128Mi
      requests:
        memory: 32Mi
    prometheus:
      monitor:
        enabled: true
        relabelings:
          - action: replace
            regex: (.*)
            replacement: $1
            sourceLabels: ["__meta_kubernetes_pod_node_name"]
            targetLabel: kubernetes_node
  prometheus-node-exporter:
    fullnameOverride: node-exporter
    prometheus:
      monitor:
        enabled: true
        relabelings:
          - action: replace
            regex: (.*)
            replacement: $1
            sourceLabels: ["__meta_kubernetes_pod_node_name"]
            targetLabel: kubernetes_node

  prometheus:
    ingress:
      servicePort: 8081  # oauth-proxy
    route:
      main:
        enabled: true
        parentRefs:
          - name: internal
            namespace: kube-system
            sectionName: https
        filters:
          - type: ResponseHeaderModifier
            responseHeaderModifier:
              add:
                - name: Content-Security-Policy
                  value: >-
                    default-src 'self' 'unsafe-inline' data:; object-src 'none';
    service:
      additionalPorts:
        - name: oauth2-proxy
          port: 8081
          targetPort: 8081
          protocol: TCP
    prometheusSpec:
      podMonitorSelectorNilUsesHelmValues: false
      probeSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false
      scrapeConfigSelectorNilUsesHelmValues: false
      serviceMonitorSelectorNilUsesHelmValues: false
      retention: 14d
      retentionSize: 50GB
      resources:
        requests:
          cpu: 100m
        limits:
          memory: 4096Mi
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: ceph-block
            resources:
              requests:
                storage: 50Gi
      additionalConfig:
        dnsConfig:
          # ensure FQDNs are not expanded with search domains, so cilium can apply policies correctly
          options:
            - name: ndots
              value: "1"

      containers:
        - name: oauth-proxy
          image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0@sha256:fe8f76728c36536d498b2a5c11172cb18dc30bbb555df2a6d14c036b29cb9f10
          envFrom:
            - secretRef:
                name: prometheus-oidc-secret

          args:
            - --upstream=http://127.0.0.1:9090
            - --http-address=0.0.0.0:8081
            - --metrics-address=0.0.0.0:8082
          ports:
            - containerPort: 8081
              name: oauth-proxy
              protocol: TCP
            - containerPort: 8082
              name: oauth-metrics
              protocol: TCP

  additionalPrometheusRulesMap:
    oom-rules:
      groups:
        - name: oom
          rules:
            - alert: OomKilled
              annotations:
                summary: >-
                  Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled
                  {{ $value }} times in the last 10 minutes.
              expr: >-
                (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1)
                and ignoring (reason)
                min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
              labels:
                severity: critical
